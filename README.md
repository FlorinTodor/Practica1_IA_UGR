# ğŸ¤– PrÃ¡ctica 1 â€“ Agente Reactivo

**Asignatura:** Inteligencia Artificial â€“ Curso 2023/2024  

---

## ğŸ¯ 1. IntroducciÃ³n
Esta prÃ¡ctica se centra en el diseÃ±o e implementaciÃ³n de un **agente reactivo** capaz de operar de forma autÃ³noma en el entorno simulado de *Los mundos de BelKan*. El objetivo principal es que el agente explore el mapa, identifique diferentes tipos de terreno, evite obstÃ¡culos y se posicione correctamente, con el fin de maximizar el porcentaje del mapa descubierto.

---

## ğŸ—ºï¸ 2. Escenario
El mundo es una matriz de hasta **100x100 celdas** que contiene diversos terrenos y elementos:

-   **Terrenos:**
    -   `B`: Bosque
    -   `A`: Agua
    -   `P`: Precipicio
    -   `T`: Arenoso
    -   `S`: Pedregoso
    -   `M`: Muro

-   **Objetos Especiales:**
    -   `K`: Bikini (permite nadar en Agua)
    -   `D`: Zapatillas (reduce coste en Bosque)
    -   `X`: Recarga de BaterÃ­a
    -   `G`: Posicionamiento (recalibra sensores)

-   **Agentes MÃ³viles:**
    -   `a`: Aldeanos
    -   `l`: Lobos
    -   `j`: Jugador (nuestro agente)

---

## ğŸ’¡ 3. CaracterÃ­sticas del Agente

### Sensores
El agente percibe el entorno a travÃ©s de un conjunto de sensores que le informan sobre:
- Terreno, agentes cercanos, colisiones, estado de `reset`.
- PosiciÃ³n, orientaciÃ³n, nivel de baterÃ­a, nivel actual y tiempo restante.

### Acciones
El agente puede ejecutar las siguientes acciones, cada una con un coste de baterÃ­a variable:
- `actWALK`: Avanzar 1 casilla.
- `actRUN`: Avanzar 2 casillas.
- `actTURN_SR`: Girar 45Â° a la derecha.
- `actTURN_L`: Girar 90Â° a la izquierda.
- `actIDLE`: No hacer nada (coste mÃ­nimo).

---

## ğŸ’» 4. ImplementaciÃ³n
El nÃºcleo del comportamiento se define en los ficheros `jugador.cpp` y `jugador.hpp`, dentro de la clase `ComportamientoJugador`. La lÃ³gica se basa en un conjunto de **reglas reactivas** que responden a estÃ­mulos inmediatos:

-   ğŸ” **DetecciÃ³n de ObstÃ¡culos:** Lee el vector de terreno para identificar y evitar obstÃ¡culos inmediatos (`M`, `P`).
-   ğŸ§­ **Estrategia de ExploraciÃ³n:** Prioriza el movimiento hacia casillas desconocidas (`?`) para maximizar la cobertura.
-   ğŸ§± **EvasiÃ³n de Muros:** Implementa una lÃ³gica de seguimiento de paredes para evitar quedarse atascado.
-   ğŸ”‹ **GestiÃ³n de BaterÃ­a:** Optimiza el uso de bikini y zapatillas para minimizar el consumo de energÃ­a en terrenos costosos.
-   ğŸ“ **AutocalibraciÃ³n:** En niveles sin sensores de posiciÃ³n/orientaciÃ³n, utiliza las casillas `G` para recalibrar su estado interno.

El agente **no emplea planificaciÃ³n global ni bÃºsqueda heurÃ­stica**; todas sus decisiones se basan en el estado actual de sus sensores.

---

## ğŸ§ª 5. Pruebas y EvaluaciÃ³n
El comportamiento del agente ha sido evaluado en distintos niveles con desafÃ­os incrementales:

-   **Nivel 0:** ExploraciÃ³n sistemÃ¡tica con sensores completos. Se logra una cobertura muy alta.
-   **Nivel 1:** DesafÃ­o sin sensor de posiciÃ³n. El agente ajusta su rumbo basÃ¡ndose en referencias visuales y obstÃ¡culos.
-   **Nivel 2:** EvasiÃ³n activa de lobos y aldeanos.
-   **Nivel 3:** GestiÃ³n con orientaciÃ³n inicial desconocida y sensores potencialmente defectuosos.

---

## ğŸ“Š 6. Resultados
El agente logra un **alto porcentaje de exploraciÃ³n** en todos los niveles, demostrando una especial eficacia en los niveles 0 y 1. En escenarios con enemigos, las tÃ¡cticas de evasiÃ³n son funcionales, aunque los reinicios forzados por colisiones pueden reducir la eficiencia global.

---

## ğŸ 7. Conclusiones
-   El enfoque **reactivo simple** permite un comportamiento Ã¡gil y muy adaptable en entornos dinÃ¡micos y parcialmente observables.
-   La **ausencia de memoria global** limita la optimizaciÃ³n de rutas, pero a cambio proporciona una alta tolerancia a cambios locales inesperados.
-   Como mejora futura, se podrÃ­a implementar un **mapa auxiliar** simple para recordar zonas ya visitadas y trazar rutas mÃ¡s seguras, especialmente en los niveles mÃ¡s complejos.
